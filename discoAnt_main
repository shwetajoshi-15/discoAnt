#!/usr/bin/env bash

# get directory of discoAnt script
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )


# source parameters file
source ${1}

echo \
"
██████  ██ ███████  ██████  ██████   █████  ███    ██ ████████ 
██   ██ ██ ██      ██      ██    ██ ██   ██ ████   ██    ██    
██   ██ ██ ███████ ██      ██    ██ ███████ ██ ██  ██    ██    
██   ██ ██      ██ ██      ██    ██ ██   ██ ██  ██ ██    ██    
██████  ██ ███████  ██████  ██████  ██   ██ ██   ████    ██ 
"
# update to v3?
echo "v2.1"

# add section here to check if the $OUTPUT_NAME directory already exists, if it does, exit
if [ -d $OUTPUT_NAME ] 
then
	echo "$OUTPUT_NAME already exists"
	exit 1  # Exit the script with a non-zero status code (1)
fi

# make directories
mkdir -p $OUTPUT_NAME
mkdir -p $OUTPUT_NAME/samples
mkdir -p $OUTPUT_NAME/mapped_data
mkdir -p $OUTPUT_NAME/bambu
mkdir -p $OUTPUT_NAME/salmon_quants
mkdir -p $OUTPUT_NAME/updated_transcriptome
mkdir -p $OUTPUT_NAME/annotated_isoforms
mkdir -p $OUTPUT_NAME/temp_files

# filter reference files and index reference genome
echo "Running on reads in: $FASTA"
echo "Indexing and filtering reference files..."

# GTF
grep $ENSG_ID $ANNA_GTF > $OUTPUT_NAME/temp_files/filt_chr.gtf
ANNA_GTF_filt=$OUTPUT_NAME/temp_files/filt_chr.gtf

# need to do #
# use filtered GTF to extract chromosome to filter FASTA, remove from parameters file
# FASTA
samtools faidx $REF_GENOME_FN $chr -o $OUTPUT_NAME/temp_files/filt_chr.fa
REF_GENOME_FN_filt=$OUTPUT_NAME/temp_files/filt_chr.fa

# Define logs file
log_file="$OUTPUT_NAME/discoAnt.logs"
# Function to redirect output to the log file
redirect_output() {
    # Redirect both stdout (1) and stderr (2) to the log file
    {
        "$@"
    } >> "$log_file" 2>&1
}

function check_for_fastq_or_fasta_files() {

	if [ -d "$FASTA" ] 
	then
        
        # Use find to search for FASTQ or FASTA files
        if find "$FASTA" -maxdepth 2 -type f \( -name "*.fastq"  -o -name "*.fastq.gz" -o -name "*.fasta" -o -name "*.fa" \) -print -quit | grep -q . 
		then
            :
			# Call the next function or perform the next task here
        else
            echo "No FASTQ or FASTA files found within subdirectories of $FASTA"
            exit 1  # Exit the script with a non-zero status code
        fi

    else
        echo "Directory not found: $FASTA"
        exit 1  # Exit the script with a non-zero status code
    fi
    
}

function concat_files() {
	
	mkdir $OUTPUT_NAME/samples/reads

	# run on all subdirectories in $FASTA
	find "$FASTA" -mindepth 1 -maxdepth 1 -type d | while read -r dir
	do

		# Use basename to extract just the directory name
		dir_name=$(basename "$dir")

		# if BBMap works on gzipped files this may not be required, minimap2 works on gzipped files
		# Gunzip zipped files
		for gzipped_file in "$FASTA/$dir_name"/*.gz 
		do
			if [ -f "$gzipped_file" ] 
			then
				#echo "Unzipping $gzipped_file..."
				gunzip "$gzipped_file"
			fi
		done
		
		# combine all sample read files into one file
		cat $FASTA/$dir_name/*.fa* > $OUTPUT_NAME/samples/reads/$dir_name.fa

	done

	# report metrics
	num_of_barcodes=$( ls -lh $OUTPUT_NAME/samples/reads/*.f* | wc -l )
	total_reads_input=$(cat $OUTPUT_NAME/samples/reads/*.f* | grep "^[>@]" | wc -l)
    
}

function downsampling_function() {
	if [ -z "$downsampling" ]
	then
    		downsampling=TRUE # Set to the default value if empty
  	fi

	if [ "$downsampling" == TRUE ]
	then
		
		if [ -z "$number_reads_downsample" ]
		then
    		number_reads_downsample="8000" # Set to the default value if empty
  		fi

		echo "Downsampling reads..."

		mkdir -p $OUTPUT_NAME/samples/downsample_$number_reads_downsample

		downsample_for_loop() { 
			for filename in $OUTPUT_NAME/samples/reads/*.fa
			do

				base=$(basename "$filename")
				sample_name="${base%.*}" 
				echo "$sample_name downsampling"
				## Downsample the reads
				## reformat.sh is from BBMap
				reformat.sh sample=$number_reads_downsample \
				in=$OUTPUT_NAME/samples/reads/$sample_name.fa \
				out=$OUTPUT_NAME/samples/downsample_$number_reads_downsample/$sample_name.fa

			done
		}
		
		redirect_output downsample_for_loop

		path_to_reads=$OUTPUT_NAME/samples/downsample_$number_reads_downsample

		total_reads_post_downsample=$(cat $OUTPUT_NAME/samples/downsample_$number_reads_downsample/*.f* | grep "^[>@]" | wc -l)
	
	elif [ "$downsampling" == FALSE ]
	then

		path_to_reads=$OUTPUT_NAME/samples/reads

	fi


}

function mapping_genome_function() {
  # define max intron length for minimap2
  if [ -z "$max_intron_length" ] 
  then
    max_intron_length="400" # Set to the default value if max_intron_length is empty
  fi

  if [ -z "$splice_flank" ] 
  then
    splice_flank="yes" # Set to the default value
  fi
  
  echo "Mapping reads..."
  for filename in "$path_to_reads"/*.fa
  do

    base=$(basename "$filename")
    sample_reads="${base%.*}" 
    
	map_reads_minimap2() {
    	minimap2 -ax splice -G"${max_intron_length}"k --splice-flank="$splice_flank" --eqx $REF_GENOME_FN_filt $path_to_reads/${sample_reads}.fa | samtools view -bh > $OUTPUT_NAME/mapped_data/${sample_reads}.bam
	}

	redirect_output map_reads_minimap2
	
  	samtools view -h -F 2308 $OUTPUT_NAME/mapped_data/${sample_reads}.bam | samtools sort - > $OUTPUT_NAME/mapped_data/${sample_reads}_primary_sorted.bam

  done

  samtools merge -f $OUTPUT_NAME/mapped_data/${OUTPUT_NAME}_primary_merged.bam $OUTPUT_NAME/mapped_data/*sorted.bam
  samtools index $OUTPUT_NAME/mapped_data/${OUTPUT_NAME}_primary_merged.bam

  if [ -e "$OUTPUT_NAME/mapped_data/${OUTPUT_NAME}_primary_merged.bam.bai" ]
  then 
	:
  else
	echo "Failed to create BAM index, check logs file"
	exit 1
  fi

  # counts total reads in a BAM file, for report?
  number_reads_mapped=$(samtools view $OUTPUT_NAME/mapped_data/${OUTPUT_NAME}_primary_merged.bam | awk '{print $1}' | sort | uniq | wc -l)

}

function run_bambu_function() {
	echo "Identifying novel isoforms with Bambu..."

	# if bambu is the latest version, NDR has to be in opts list, if not, NDR is its own arg
	redirect_output Rscript $SCRIPT_DIR/scripts/bambu_tx_discovery.R -b $OUTPUT_NAME/mapped_data/${OUTPUT_NAME}_primary_merged.bam \
		-f $REF_GENOME_FN_filt \
		-t $ANNA_GTF_filt \
		-o $OUTPUT_NAME/bambu

	if [ -e "$OUTPUT_NAME/bambu/extended_annotations.gtf" ]
	then
		if [ -s "$OUTPUT_NAME/bambu/extended_annotations.gtf" ]
		then
			:
		else
			echo "Failed to create Bambu annotations, check logs file"
			exit 1
		fi
	else
		echo "Failed to create Bambu annotations, check logs file"
		exit 1
	fi

}

function read_count_function_first_pass() {
	if [ -z "$read_count_minimum" ]
	then
    	read_count_minimum="5" # Set to the default value if empty
  	fi
	
	# same read count > 1 threshold  
	cat "$OUTPUT_NAME/bambu/counts_transcript.txt" | awk -v min_count="$read_count_minimum" '{ if ($3 > min_count ) print $1 }' | tail -n +2 > "$OUTPUT_NAME/temp_files/transcripts_combined_counts_1_list.txt"

	
	# subset bambu GTF for the transcripts that passed threshold
	cat $OUTPUT_NAME/bambu/extended_annotations.gtf | grep -wf $OUTPUT_NAME/temp_files/transcripts_combined_counts_1_list.txt > $OUTPUT_NAME/bambu/extended_annotations_count_1.gtf
	
	# replace some Bambu naming conventions
	cat $OUTPUT_NAME/bambu/extended_annotations_count_1.gtf | sed 's/tx./tx/g' | sed 's/BambuTx/tx/g' > $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf
	cat $OUTPUT_NAME/temp_files/transcripts_combined_counts_1_list.txt | sed 's/tx./tx/g' | sed 's/BambuTx/tx/g' | awk '{ print $1"\t"$3}' > $OUTPUT_NAME/temp_files/updated_transcriptome_count_1.txt

	# check file exists and is not empty
	if [ -e "$OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf" ]
	then 	
		if [ -s "$OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf" ]
		then
			:
		else
			echo "Read count threshold first pass failed, check logs file"
			exit 1
		fi
	else
		echo "Read count threshold first pass failed, check logs file"
		exit 1
	fi

}

function primer_site_function() {
	
	if [ -z "$primer_site_based_filter" ]
	then
    		primer_site_based_filter=FALSE # Set to the default value if empty
  	fi

	if [ "$primer_site_based_filter" == TRUE ]
	then
		echo "Filtering based on primer BED files..."
		# used to be a 5bp window created around primers, do we need this..?
		
		# intersect forward and reverse primers with GTF
		cat $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf | awk '{ if ($3 == "exon") print }' | bedtools intersect -wa -u -a - -b $forward_primers | grep -o 'transcript_id "[^"]*' | cut -d' ' -f2 | sed 's/"//g' > $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_forward_primer_exons.txt
		cat $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf | awk '{ if ($3 == "exon") print }' | bedtools intersect -wa -a - -b $reverse_primers | grep -o 'transcript_id "[^"]*' | cut -d' ' -f2 | sed 's/"//g' > $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_reverse_primer_exons.txt
		
		# combine isoforms found in both forward and reverse primers
		comm -12 <(sort $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_forward_primer_exons.txt) <(sort $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_reverse_primer_exons.txt) > $OUTPUT_NAME/temp_files/updated_transcriptome.txt
		
		# subset GTF that passed counts filtering with new list of transcripts that match primers
		cat $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf | grep -wf $OUTPUT_NAME/temp_files/updated_transcriptome.txt > $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_pass1.gtf

		if [ -e "$OUTPUT_NAME/temp_files/updated_transcriptome.txt" ]
		then 	
			if [ -s "$OUTPUT_NAME/temp_files/updated_transcriptome.txt" ]
			then
				:
			else
				echo "Failed to filter based on primers, check logs file"
				exit 1
			fi
		else
			echo "Failed to filter based on primers, check logs file"
			exit 1
		fi

		# Count the number of isoforms kept after primer filtering \
		count_file1=$(awk '$3 == "transcript" {count++} END {print count}' "$OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_pass1.gtf")
		count_file2=$(awk '$3 == "transcript" {count++} END {print count}' "$OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf")

		# Calculate 50% of the lines in file2
		primer_threshold=$(printf "%.0f" "$(echo "$count_file2 * 0.5" | bc)")

		# Check if count_file1 is greater than half_count_file2
		if [ "$count_file1" -lt "$primer_threshold" ] 
		then
			echo "WARNING: more than 50% of the isoforms identified with Bambu were removed after primer site filtering"
		fi

	elif [ "$primer_site_based_filter" == FALSE ]
	then
		cat $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_count_1.gtf > $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_pass1.gtf
		cat $OUTPUT_NAME/temp_files/updated_transcriptome_count_1.txt > $OUTPUT_NAME/temp_files/updated_transcriptome.txt
	fi
}

function create_metatranscriptome() {
	echo "Creating an updated transcriptome with novel isoforms..."

	redirect_output gffread -w $OUTPUT_NAME/updated_transcriptome/updated_transcriptome.fa -g $REF_GENOME_FN_filt $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_pass1.gtf
	
	if [ -e "$OUTPUT_NAME/updated_transcriptome/updated_transcriptome.fa" ]
  	then 
		:
 	else
		echo "Failed to create updated transcriptome with gffread, check logs file"
		exit 1
  	fi

	redirect_output salmon index -t $OUTPUT_NAME/updated_transcriptome/updated_transcriptome.fa -i $OUTPUT_NAME/updated_transcriptome -k 31
	
	if [ -e "$OUTPUT_NAME/updated_transcriptome/versionInfo.json" ]
  	then 
		:
 	else
		echo "Failed to index with salmon, check logs file"
		exit 1
  	fi
}

function remapping_function() {
	echo "Mapping reads..."
	running_salmon() {
		for filename in $path_to_reads/*.fa
		do

			base=$(basename $filename)
			sample_reads="${base%.*}"
			
			salmon quant --quiet -i $OUTPUT_NAME/updated_transcriptome -l A -r $path_to_reads/${sample_reads}.fa -o $OUTPUT_NAME/salmon_quants/${sample_reads}

		done
	}

	redirect_output running_salmon
		
	if [ -d "$OUTPUT_NAME/salmon_quants" ]
  	then
		if [ "$(ls -A $OUTPUT_NAME/salmon_quants)" ] 
		then
     		:
		else
    		echo "Failed to quantify with salmon, check logs file"
			exit 1
		fi
 	else
		echo "Failed to quantify with salmon, check logs file"
		exit 1
  	fi

}

function combine_quants_function() {
	if [ -z "$read_count_minimum" ]
	then
    	read_count_minimum="5" # Set to the default value if empty
  	fi

	if [ -z "$samples_minimum" ]
	then
		samples_minimum=$(echo "scale=0; $num_of_barcodes/2" | bc -l)
  	fi
	
	echo "Generating isoform counts..."
	Rscript $SCRIPT_DIR/scripts/combine_salmon_quants.R -e $ENSG_ID -m $read_count_minimum -s $samples_minimum -o $OUTPUT_NAME

	reads_remaining_final=$(cat $OUTPUT_NAME/temp_files/remaining_read_sum.txt)

}

function read_count_function_second_pass() {

	# create list in text file of isoforms that passed threshold
	# command uses _ and , as sep, prints first column of tx names, removes the word Bambu in tx names, and removes the column header
	cat $OUTPUT_NAME/${OUTPUT_NAME}_counts.csv | awk -F '[_,]' '{print $1}' | sed 's/Bambu//g' | tail -n +2 > $OUTPUT_NAME/temp_files/transcripts_read_counts_second_pass_list_tmp.txt
	
	# filter GTF based on isoforms 
	cat $OUTPUT_NAME/temp_files/${OUTPUT_NAME}_isoforms_pass1.gtf | grep -wif $OUTPUT_NAME/temp_files/transcripts_read_counts_second_pass_list_tmp.txt > $OUTPUT_NAME/${OUTPUT_NAME}_isoforms.gtf

	# report metrics
	filtered_transcripts_known=$( cat $OUTPUT_NAME/temp_files/transcripts_read_counts_second_pass_list_tmp.txt | grep -vi tx | wc -l )
	filtered_transcripts_novel=$( cat $OUTPUT_NAME/temp_files/transcripts_read_counts_second_pass_list_tmp.txt | grep -i tx | wc -l )

	if [ -e "$OUTPUT_NAME/${OUTPUT_NAME}_isoforms.gtf" ]
	then 	
		if [ -s "$OUTPUT_NAME/${OUTPUT_NAME}_isoforms.gtf" ]
		then
			:
		else
			echo "Read count threshold second pass failed, check logs file"
			exit 1
		fi
	else
		echo "Read count threshold second pass failed, check logs file"
		exit 1
	fi

}

function gffcomp_function() {
	redirect_output gffcompare -r $ANNA_GTF_filt -o $OUTPUT_NAME/annotated_isoforms/gffcomp $OUTPUT_NAME/${OUTPUT_NAME}_isoforms.gtf
	
	mv $OUTPUT_NAME/gffcomp.* $OUTPUT_NAME/annotated_isoforms/

}

function sqanti_function() {
 	# source python paths for cDNA Cupcake to run with SQANTI
  	export PYTHONPATH="$SCRIPT_DIR/scripts/cDNA_Cupcake/sequence/:$PYTHONPATH"
  	export PYTHONPATH="$SCRIPT_DIR/scripts/cDNA_Cupcake/:$PYTHONPATH"
  
		
	python $SCRIPT_DIR/scripts/SQANTI3/sqanti3_qc.py \
		$OUTPUT_NAME/${OUTPUT_NAME}_isoforms.gtf \
		$ANNA_GTF_filt $REF_GENOME_FN_filt \
		-d $OUTPUT_NAME/sqanti -o $OUTPUT_NAME/annotated_isoforms --report skip
		#--CAGE_peak $REF_HG38/refTSS_v3.3_human_coordinate.hg38.bed \
		#--polyA_peak $REF_HG38/atlas.clusters.2.0.GRCh38.96.bed --polyA_motif_list $REF_HG38/human.polyA.list.txt

}

function generate_report_function() {
cat << EOT >> $OUTPUT_NAME/${OUTPUT_NAME}_report.txt
`date`

Number of input samples/barcodes: $num_of_barcodes
Number of reads across barcodes: $total_reads_input

Filters applied:
	Primer site filter: $primer_site_based_filter
	Downsampling: $downsampling
	Downsampled to number of reads: $number_reads_downsample
	Read count minimum: $read_count_minimum
	Samples required to meet read count minimum: $samples_minimum

Number of reads across barcodes post-downsampling: $total_reads_post_downsample
Number of reads mapped: $number_reads_mapped

Known isoforms identified: $filtered_transcripts_known
Novel isoforms identified: $filtered_transcripts_novel

Reads mapped to known/novel isoforms: $reads_remaining_final

EOT
}


########## 0. Counting the number of reads and downsampling ##########

# check filetypes
check_for_fastq_or_fasta_files
concat_files

# Downsampling
downsampling_function


########## 1. Aligning sample fasta files to reference genome ##########

# run minimap2
mapping_genome_function


########## 2.a. Correcting and collapsing transcripts with bambu ##########

# run bambu in R
run_bambu_function


########## 2.b. Filtering bambu transcripts and creating a transcriptome ##########

# Read count filter 
read_count_function_first_pass

# Primer site filter
primer_site_function

# metatranscriptome with salmon
create_metatranscriptome


########## 2.c. Re-aligning and quantifying filtered bambu transcripts  ##########

# remap to salmon metatranscriptome
remapping_function


########## 2.d. Generating count matrix   ##########

# Rscript to combine quant.sf files and filter by read min counts
combine_quants_function

# some zero count isoforms result after the secound round of quants with salmon
read_count_function_second_pass


########## 3. Annotating transcripts ##########
echo "Annotating isoforms..."
# run with gffcompare
gffcomp_function

# run with SQANTI
#sqanti_function


########## 4. Report ##########

# make report txt file
echo "Generating report..."
generate_report_function

# removing temp files
# currently commented out while de-bugging
#rm -rf $OUTPUT_NAME/temp_files

########## Finished ##########
echo "Complete"


